\newpage
\section{Consistency and Nonparametric Algorithms}

\begin{itemize}

    \item Write the mathematical definition of consistency for an algorithm $A$.\\ 
        
        A learning algorithm $A$ is consistent with respect to a loss function $\ell$ if for any data
distribution $D$ it holds that
        $$
        \lim_{m \to \infty} \mathbb{E} \left[ \ell_{\mathcal{D}}(A(S)) \right] = \ell_{\mathcal{D}}(f^*)
        $$
        where the expectation is with respect to the random draw of the training set $S_m$ of size $m$ from the distribution $\mathcal{D}$, and $\ell_{\mathcal{D}}(f)$ is the Bayes risk risk for $(\mathcal{D}, f)$.\\

    \item Write the statement of the no-free-lunch theorem.\\
        
        For any sequence $a_1, a_2, \dots$ of positive numbers converging to zero and such that $\frac{1}{16} \geq a_1 \geq a_2 \geq \dots$ and for any consistent learning algorithm $A$ for binary classification with zero-one loss, there exists a data distribution $\mathcal{D}$ such that $\ell_{\mathcal{D}}(f^*) = 0$ and $\mathbb{E}[\ell_{\mathcal{D}}(A(S_m))] \geq a_m$ for all $m \geq 1$.\\

    \item Write the mathematical definition of nonparametric learning algorithm. Define the main quantities occurring in the formula.\\

        Given a learning algorithm $A$, let $\mathcal{H}_m$ be the set of predictors generated by $A$ on training sets of size $m: h \in \mathcal{H}_m$ if and only if there exists a training set $S_m$ of size $m$ such that $A(S_m) = h$. We say that $A$ is a nonparametric learning algorithm if $A$'s approximation error vanishes as $m$ grows to infinity. Formally,
        $$
        \lim_{m \to \infty} \min_{h \in \mathcal{H}_m} \ell_{\mathcal{D}}(h^*) = \ell_{\mathcal{D}}(f^*) 
        $$

    \item Name one nonparametric learning algorithm and one parametric learning algorithm.\\

        Nonparametric: k-nearest neighbors and greedy decision tree classifiers\\
        Parametric: linear regression and logistic regression\\

    \item Write the mathematical conditions on $k$ ensuring consistency for the $k$-NN algorithm.\\

    We let $k$ be chosen as a function $k_m$ of the training set size, then the algorithm becomes consistent provided $k_m \rightarrow \infty$ and $k_m = o(m)$.\\
    Two possible example are $k_m = \sqrt{m}$ and $k_m = \log{m}$\\

    \item Write the formula for the Lipschitz condition in a binary classification problem. Define the main quantities occurring in the formula.\\

        In some cases, we may define consistency with respect to a restricted class of distributions $\mathcal{D}$. For example, in binary classification we may restrict to all distribution $\mathcal{D}$ such that $\eta(\boldsymbol{x}) = \mathbb{P}(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$ is Lipschitz function on $\mathcal{X}$. Formally, there exists $0 < c < \infty$ such that
        \begin{equation} 
            \begin{aligned}
                |\eta(\boldsymbol{x}) - \eta(\boldsymbol{x}')| \leq c \|\boldsymbol{x} - \boldsymbol{x}'\| && \text{for all $\boldsymbol{x}, \boldsymbol{x}' \in \mathcal{X}$} \\ 
            \end{aligned}
        \end{equation}

    \item Write the rate at which the risk of a consistent learning algorithm for binary classification vanish as a function of the training set size $m$ and the dimension $d$ under Lipschitz assumptions.\\

        Under Lipschitz assumptions on $\eta$, the typical convergence rate to Bayes risk is $m^{\frac{-1}{d+1}}$.\\

    \item Explain the curse of dimensionality.\\
    
        The convergence rate $m^{\frac{-1}{d+1}}$ implies that to get $\epsilon$-close to the Bayes risk, we need a training set size $m$ of order $\epsilon^{-(d+1)}$. This exponential dependence on the number of features of the training set size is known as \textbf{curse of dimensionality} and refers to the difficulty of learning in a nonparametric setting when datapoints live in a high-dimensional space.\\
    

\end{itemize}
