\newpage
\section{Boosting and ensemble methods}

\begin{itemize}

    \item Explain how the Bagging Algorithm works.\\

    Fix a training set $S = \{(x_1 , y_1),\dots,(x_m , y_m)\}$ of size $m$ for a binary classification problem with zero-one loss, and 
    assume an ensemble of classifiers $h_1, \dots , h_T$ is available.\\
    Consider the majority classifier $f$ defined by $$f(x) = sgn(\sum_{i=1}^{T} {h_i(x)})$$
    If the training error of each classifier $l_s(h_i)$ in the ensemble is indipendent from each other we can achive a training error on $f$ that decreases exponentially in $T\gamma^2$ (Look at the next question).\\
    Bagging is an heuristic that applies to any learning algorithm for binary classification A.\\ 
    Bagging creates $T$ variants of the training set $S$: $S_1, S_2, \dots S_T$ sampling $m$ data points uniformly at random for each of them.\\
    For each training set just constructed Bagging creates a classifier $h_i = A(S_i)$.\\
    The idea is that the resampling procedure helps enforce the condition of training errors indipendece.\\

    \item Write the upper bound on the training error achieved by Bagging when the errors of the classifiers are independent. Define the main quantities occurring in the bound.\\

        When the tranining errors of the classifiers are indipendent Bagging obtains an upper bound on the majority vote classifier of:
        $$l_s(f) \leq e^{-2T\gamma^2}$$
        Where
        \begin{itemize}
            \item $l_s(f)$ is the training error for the classifier $f$ on the training set $S = \{(x_1 , y_1),\dots,(x_m , y_m)\}$.\\
            \item $T$ is the number of classifiers in the ensemble.\\
            \item $\gamma = min_{i=1 \dots T} \gamma_i$ where  $\gamma_i = \frac{1}{2} - l_s(h_i)$ or intuitively how much better each predictor $h_i$ respect to the random one.\\
        \end{itemize}
    
    \newpage
    \item Explain how the AdaBoost algorithm works  .\\
        
        \begin{algorithm}[H]
            \SetAlgoLined
            \DontPrintSemicolon
            \caption{AdaBoost}
            \KwIn{Training set $S = (\boldsymbol{x}_1 , y_1 ), \dots, (\boldsymbol{x}_m , y_m)$,\\ A learning Algorithm $A$,\\ Maximum number of rounds of boosting $T$}
            $\mathbb{P}_1(t) \leftarrow \frac{1}{m}$ for all $t = 1 \dots m$\\
            \For{$i = 1, \dots, T$}{
                1) create a predictor $h_i$ with the algorithm $A$ using the training set $S$ weighted by $\mathbb{P}_i$\\
                2) compute the error $\varepsilon_i$ for $h_i$\\
                3) \uIf{$\varepsilon_i \in {1, 1/2, 0}$} {
                    \textbf{break}
                } 
                4) compute the weight $w_i = \frac{1}{2} \ln(\frac{1 - \varepsilon_i}{\varepsilon_i})$\\
                5) compute the probability for the next predictor $\mathbb{P}_{i+1}(t) = \frac{\mathbb{P}_i(t) e^{-w_i L_i(t)}}{\mathbb{E}_i\left[e^{w_i L_i(Z_i)}\right]}$ for all $t=1 \dots m$\\    
            }
            \uIf{exit on error break} {
                Deal with the special case: if is either 0 or 1 return $h_i$ or $-h_i$ because they alone can zeroing the training set.\\
                Otherwise the boosting procedure is stuck beacuse it reach a point where it cannot improve the learning algorithm anymore.\\
            } 
        \uElse output {$f = sgn(\sum_{i=1}^T w_i h_i$})
        \end{algorithm}

        Where the in point 5) the expectation is regard to the random draw of an index of the training sample $Z_i$.\\ 
        Boosting can be seen as a sequence of round where the booster give $\mathbb{P}_i$ to the algorithm $A$ and receive a predictor $h_i$ in response.\\
        Note that when we are calculating the weight $w_i$ if $\varepsilon_i \geq \frac{1}{2}$ the log is negative and so we are assigning a negative weight to that predictor (we are flipping his prediction).\\ 
        
    \item Write the upper bound on the training error achieved by AdaBoost. Define the main quantities occurring in the bound.\\
        
        AdaBoost can achieve a bound on the training error of $$l_s(f) \leq e^{-2T\gamma^2}$$ like Bagging, but differently from it AdaBoost doesn't 
        require the predictors in the ensemble to have indipendent training errors. 
        Recall that 
        \begin{itemize}
            \item $l_s(f)$ is the training error for the classifier $f$ on the training set $S = \{(x_1 , y_1),\dots,(x_m , y_m)\}$.\\
            \item $T$ is the number of classifiers in the ensemble.\\
            \item $\gamma = min_{i=1 \dots T} \gamma_i$ where  $\gamma_i = \frac{1}{2} - l_s(h_i)$ so how much better each predictor $h_i$ respect to the random one.\\
        \end{itemize}
        
    \item How many rounds of AdaBoost are sufficient to achieve zero training error under the condition $|\gamma_i| \geq \gamma$ for all $i$?\\
        
        Note that is not possible to achieve a training error $l_s(f) < \frac{1}{m}$ unless $l_s(f) = 0$.\\
        Recall the bound $$l_s(f) \leq e^{-2T\gamma^2}$$ on the training error for AdaBoost (where $T$ is the numer of round and $\gamma_i$ how better 
        each predictor $h_i$ performs compared to the random one), 
        if $l_s(f) = 0$ we can write $$e^{-2T\gamma^2} < \frac{1}{m}$$
        Solving for T we have
        $$-2T\gamma^2 < \ln{\frac{1}{m}}$$
        $$2T\gamma^2 > \ln(m)$$
        $$T > \frac{\ln(m)}{2\gamma^2}$$

        So $\frac{\ln(m)}{2\gamma^2}$ rounds of AdaBoost are sufficent to achive zero training error.
        
    \end{itemize}
    