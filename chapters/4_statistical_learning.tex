\newpage
\section{Statistical Learning}

\begin{itemize}
    
    \item  Write the formula for the statistical risk of a predictor $h$ with respect to a generic loss function and data distribution.\\

        The performance of a predictor $h : \mathcal{X} \rightarrow \mathcal{Y}$ with respect to $(\mathcal{D}, \ell)$ is evluated via the \textbf{statistical risk}, defined by:
        $$
            \ell_{\mathcal{D}}(h) = \mathbb{E}_{(\boldsymbol{x}, y) \sim \mathcal{D}}[\ell(Y, h(\boldsymbol{X}))] 
        $$
        This is the expected value of the loss function on a randon example $(\boldsymbol{X}, Y)$ drawn from $\mathcal{D}$.\\

    \item Write the formula for the Bayes optimal predictor for a generic loss function and data distribution.\\

        The best possible predictor $f^{*} : \mathcal{X} \rightarrow \mathcal{Y}$ given $\mathcal{D}$ is known as the \textbf{Bayes optimal predictor}, and is defined by:
        $$
        f^{*}(\boldsymbol{x}) = \underset{\hat{y} \in \mathcal{Y}}{\textmd{argmin}}\ \mathbb{E}[\ell(Y, \hat{y})\ |\ \boldsymbol{X} = \boldsymbol{x}]
        $$

    \item Write the formula for Bayes optimal predictor and Bayes risk for the zero-one loss.\\

        On binary classification, where $\mathcal{Y} = \{-1, +1\}$ . Let $\eta(\boldsymbol{x})$ be the probability of $Y = 1$ conditioned on $\textbf{X} = \boldsymbol{x}$. We view $\eta(\boldsymbol{x}) = \mathbb{P}(Y = +1\ |\ \textbf{X} = \boldsymbol{x})$ as the value on $\boldsymbol{x}$ of a function $\eta : \mathcal{X} \rightarrow [0,1]$.\\
        Let $\mathbb{I}\{A\} \in \{0,1\}$ be the indicator function of the event $A$; that is, $\mathbb{I}\{A\} = 1$ if $A$ occurs and $\mathbb{I}\{A\} = 0$ otherwise.\\
        The statistical risk with respect to the zero-one loss $\ell(y, \hat{y}) = \mathbb{I}\{y \neq \hat{y}\}$ is therefore defined by:
        $$
        \ell_{\mathcal{D}}(h) = \mathbb{E}[\mathbb{I}\{Y \neq h(\boldsymbol{X})\}] = \mathbb{P}(Y \neq h(\boldsymbol{X}))
        $$


        The Bayes optimal predictor $f^{*} : \mathcal{X} \rightarrow \{-1, +1\}$ for binary classification is derived as follows:
        \begin{equation}
            \begin{split}
                f^{*}(\boldsymbol{x}) & = \underset{\hat{y} \in \{-1,+1\}}{\textmd{argmin}}\ \mathbb{E}[\ell(Y, \hat{y})\ |\ \boldsymbol{X} = \boldsymbol{x}]\\
                                      & = \underset{\hat{y} \in \{-1,+1\}}{\textmd{argmin}}\ (\mathbb{P}(Y = +1\ |\ \boldsymbol{X} = \boldsymbol{x})\mathbb{I}\{\hat{y} = -1\} + \mathbb{P}(Y = -1\ |\ \boldsymbol{X} = \boldsymbol{x})\mathbb{I}\{\hat{y} = +1\})\\                                   
                                      & = \underset{\hat{y} \in \{-1,+1\}}{\textmd{argmin}} (\eta(\boldsymbol{x})\mathbb{I}\{\hat{y} = -1\} + (1-\eta(\boldsymbol{x}))\mathbb{I}\{\hat{y} = +1\})\\
                                      & = \begin{cases}
                                          -1 & \textmd{if}\ \eta(\boldsymbol{x}) < 1/2\\
                                            +1 & \textmd{if}\ \eta(\boldsymbol{x}) \geq 1/2                                           \end{cases}
            \end{split}
        \end{equation}
        Hence, the Bayes optimal classifier predicts the label whose probability is the highest when conditioned on the instance. Finally, it is easy to verify that the Bayes risk in this case is $\ell_{\mathcal{D}}(f^{*}) = \mathbb{E}[\textmd{min}\{\eta(\textbf{X}), 1-\eta(\textbf{X})\}]$\\

    \item Can the Bayes risk for the zero-one loss be zero? If yes, then explain how.\\

        In the case of the zero-one loss, where the penalty is 1 for each incorrect prediction and 0 for each correct prediction, a Bayes risk of zero implies that the error rate is zero.

        To achieve a Bayes risk of zero, two conditions must be met:

        \begin{enumerate}
            \item \textbf{Perfectly separable data}: The classes in the data must be completely separable without any overlap. This means that there should be a decision boundary that can perfectly separate the instances of different classes. If such a decision boundary exists, it is possible to classify all instances correctly, resulting in an error rate of zero.

            \item \textbf{Knowledge of the true underlying distribution}: To construct a decision rule that achieves a Bayes risk of zero, you would need to know the true distribution from which the data is generated. With this knowledge, you can design a decision rule that assigns each instance to its correct class with certainty, leading to a perfect classification and zero error rate.
        \end{enumerate}
        
    \item Write the formula for Bayes optimal predictor and Bayes risk for the square loss.\\

        We can compute the Bayes optimal predictor for the quadratic loss function $\ell(y, \hat{y}) = (y - \hat{y})^{2}$ when $\mathcal{Y} \equiv \mathbb{R}$\\
        \begin{equation} 
            \begin{split}
                f^{*}(\boldsymbol{x}) & = \underset{\hat{y} \in \mathbb{R}}{\textmd{argmin}}\ \mathbb{E}[(Y - \hat{y})^{2}\ |\ \boldsymbol{X} = \boldsymbol{x}]\\
                                      & = \underset{\hat{y} \in \mathbb{R}}{\textmd{argmin}}\ \left( \mathbb{E}[Y^{2}\ |\ \boldsymbol{X} = \boldsymbol{x}] + \hat{y}^{2} - 2\hat{y} \mathbb{E}[Y\ |\ \boldsymbol{X} = \boldsymbol{x}]\right)\\ 
                                      & = \mathbb{E}[Y\ |\ \boldsymbol{X} = \boldsymbol{x}]\\
            \end{split}
        \end{equation}
        The Bayes optimal prediction for the quadratic loss function is the expected value of the label conditioned on the instance.\\
        Substituting in the conditional risk formula $\ell_{\mathcal{D}}(h) = \mathbb{E}[(Y - f^{*}(\boldsymbol{X}))^{2}\ |\ \boldsymbol{X} = \boldsymbol{x}]$ the Bayes optimal predictor $f^{*}(\boldsymbol{x}) = \mathbb{E}[Y\ |\ \boldsymbol{X} = \boldsymbol{x}]$ we obtain:
        $$
        \mathbb{E}[(Y - f^{*}(\boldsymbol{X}))^{2}\ |\ \boldsymbol{X} = \boldsymbol{x}] = \mathbb{E}[(Y - \mathbb{E}[Y\ |\ \boldsymbol{x}])^{2}\ |\ \boldsymbol{X} = \boldsymbol{x}] = \textmd{Var}[Y\ |\ \boldsymbol{X} = \boldsymbol{x}]
        $$

        \item Explain in mathematical terms the relationship between test error and statistical risk.\\
            
        It should be clear that, given an arbitrary predictor $h$, we cannot directly compute its risk $\ell_{\mathcal{D}}(h)$ with respect to $D$ because $D$ is typically unknown.
        We thus consider the problem of estimating the risk of a given predictor $h$. In order to compute this estimate, we can use the \textbf{test set} $S^\prime = \{(x^\prime_{1} , y^\prime_{1}), \dots , (x^\prime_{n}, y^\prime_{n})\}$. We can then estimate $\ell_{\mathcal{D}}(h)$ with the \textbf{test error}, which is the average loss of $h$ on the test set,
        $$
        \ell_{S^\prime}(h) = \frac{1}{n}\sum_{t=1}^{n} \ell(y^\prime_{t}, h(\boldsymbol{x}^\prime_{t}))
        $$
        Under the assumption that the test set is generated through independent draws from $D$, the test error corresponds to the \textbf{sample mean} of the risk. Indeed, for each $t = 1, \dots, n$ the example $(X^\prime_{t} , Y^\prime_{t})$ is an independent draw from $D$. Therefore,
        $$
        \mathbb{E}[\ell(Y^\prime_{t}, h(\boldsymbol{X}^\prime_{t}))] = \ell_{\mathcal{D}}(h) \quad \textmd{for all}\ t = 1, \dots, n
        $$
        Note that the above equalities rely on the assumption that $h$ does not depend on the test set. If it did, then the above equalities would not be necessarily true. This fact is important in the analysis of learning algorithms.

    
    \item State the Chernoff-Hoeffding bounds.\\

        Let $Z_1, \dots, Z_n$ be indipendent and identically distributed random variables with expectation $\mu$ and such that $0 \leq Z_i \leq 1$ for all $i = 1, \dots, n$. Then, for any $\varepsilon > 0$,
        $$
        \mathbb{P}\left(\frac{1}{n}\sum_{t=1}^{n} Z_t > \mu + \varepsilon\right) \leq e^{-2n\varepsilon^{2}}
        \quad and \quad
        \mathbb{P}\left(\frac{1}{n}\sum_{t=1}^{n} Z_t < \mu - \varepsilon\right) \leq e^{-2n\varepsilon^{2}}
        $$

        Using the Chernoff-Hoeffding bound with $Z_t = \ell(y_t, h(\boldsymbol{x}_t)) \in [0, 1]$ we can compute a confidence interval for the risk as follows (where the test error is written as $\ell$ instead of $\ell_{S^\prime}$):
        \begin{equation} 
            \begin{split}
                \mathbb{P}\left(|\ell_{\mathcal{D}}(h) - \ell(h)| > \varepsilon\right) & = \mathbb{P}\left(\ell_{\mathcal{D}}(h) - \ell(h) > \varepsilon\ \cup\ \ell(h) - \ell_{\mathcal{D}}(h) > \varepsilon\right)\\ 
                                                                                    & = \mathbb{P}\left(\ell_{\mathcal{D}}(h) - \ell(h) > \varepsilon\right) + \mathbb{P}\left(\ell(h) - \ell_{\mathcal{D}}(h) > \varepsilon\right)\\ 
                                                                                    & \leq 2e^{-2n\varepsilon^{2}}\\
            \end{split}
        \end{equation}

    \item Write the bias-variance decomposition for a generic learning algorithm $A$ and associate the resulting components to overfitting and underfitting.\\

       Fix a training set $S$ and let $h_S = A(S)$. The following is called the \textbf{bias-variance decomposition}:

    \begin{equation} 
            \begin{aligned}
                \ell_{D}(h_{S}) & = \ell_{D}(h_S) - \ell_{D}(h^{*}) && \text{estimation/variance error (large when overfitting)} \\ 
                                & + \ell_{D}(h^{*}) - \ell_{D}(f^{*}) && \text{approximation/bias error (large when underfitting)} \\
                                & + \ell_{D}(f^{*}) && \text{Bayes error (unavoidable)} 
            \end{aligned}
        \end{equation}

    where $f^*$ is the Bayes optimal predictor for $(\mathcal{D}, \ell)$.\\

    \item  Write the upper bound on the estimation error of ERM run on a finite class $\mathcal{H}$ of predictors.\\
        
        We now study the case $|\mathcal{H}| < \infty$, that is when the model space contains a finite number of predictors. 
        Note that the event $\exists h \in \mathcal{H} : |\ell_{\mathcal{D}}(h) - \ell_{S}(h)| > \varepsilon / 2$ is the union over $h \in \mathcal{H}$ of the events $|\ell_{\mathcal{D}}(h) - \ell_{S}(h)| > \varepsilon / 2$. 
        Therefore, by the union bound, we have that its probability is bounded by $|\mathcal{H}|$ times the probability of the event $|\ell_{\mathcal{D}}(h) - \ell_{S}(h)| > \varepsilon / 2$ for a single predictor $h \in \mathcal{H}$ is $\leq |\mathcal{H}|2e^{-m\varepsilon^2/2}$.

        In conclusion, we have that
        $$
            \mathbb{P}(\ell_{D}(h_S) - \ell_{D}(h^*) > \varepsilon) \leq 2|\mathcal{H}|e^{-m\varepsilon^2/2}
        $$
    
        Setting the right-hand side of equal to $\delta$ and solving for $\varepsilon$ we obtain that:
        $$
        \ell_{D}(h_S) \leq \ell_{D}(h^*) + \sqrt{\frac{2}{m}\ln\frac{2|\mathcal{H}|}{\delta}}
        $$


\end{itemize}
